#math #calculus #series 

$$The\;Summit\;of\;One-Variable\;Calculus.$$  

Previously, we have a *power series*, and want to find the *corresponding function* $f$; now we have *a function* $f$, wanting to find its *power series*.  

Given a **good** function $f$, can we write it as the sum function of power series?

Consider $\sum a_{n} x^n,$ Question: $\begin{cases} 1. a_{n} = ? \\ 2. \text{Does it converge or not?}\\ 3.\text{If converges, does it }\to f(x)? \end{cases}$  
Suppose we have a **good** *(differentiable for all orders)* $f$ in some interval centered at $a.$
$$f(x) = \sum_{n=0} ^{\infty} a_{n} (x-a)^{n}, \quad x \in (a-R, a+R).$$  
Set $x=a, \; f'(a)=a_{0},$ a constant.  
Set $x=a, \; f''(a)=a_{1},$ a constant.  
*Similarly*, $f''(a)=2a_{2}, \; f^{(3)}(a)=2\times 3a_{3}.$  

By this **smooth** function, we find all order of derivatives at $x=a,$ with coefficients in power series 
$$a_{n} = \cfrac{f^{(n)}(a)}{n!}.$$ 
Therefore, if $f$ has a series representation centered at $x=a,$ the series is unique in the form of 
$$f(x)= f(a) + f'(a)(x-a) + \dots + \cfrac{f^{(n)}(a)}{n!}(x-a)^{n} + \dots$$  
This is called **Taylor Series**.  

# Definition  

> $f$ is **smooth** with all order of derivatives in some interval containing $a$ as an interior point. Then the **Taylor Series** generated by $f(x)$ at $x=a$ is given by  
> $$\sum_{k=0}^{\infty} \cfrac{f^{(k)}(a)}{k!}(x-a)^{k}.$$  
> In particular, when $a=0,$ we call it the **Maclaurin Series** of $f(x)$.  

$\quad$  
eg 1:  

Find the Maclaurin Series of $f(x)=e^{ x },\;g(x)=\cos x.$  

$\underline{Sol}\;$ 1) Since $f^{(n)}(x)=e^{ x }$ for all $n$, which gives $f^{(n)}(0)=1.$ Therefore, the 
$\quad\;$ Maclaurin Series is $$\sum_{n=0}^{\infty} \frac{x^{n}}{n!}, \quad x \in \mathbb{R}.$$  
$\quad\;$ 2) $\cos x \to -\sin x \to -\cos x \to \sin x \to \text{periodic}$  
$\quad\;$ and the even terms give 0. Therefore, the Maclaurin Series  is $$\sum_{n=0}^{\infty} \frac{(-1)^{n}x^{2n}}{2n!}, \quad x \in \mathbb{R}.$$  
$\quad\;$ Here we can show that these two Power Series converge for all $x$.  

*Remark:* Here we don't write $f(x)=$ this Power Series, because:  
1. We don't know if the power series we write down converges **other than that center**.  
2. Even if it converges in some interval, $\forall\,x$ where it converges, we don;t know if the **sum function** of this Taylor Series **is** our $f(x).$  

eg 2:  

There are smooth function whose Taylor Series converges but **NOT** equal to $f(x)$ except at the center.  
$$f(x)=\begin{cases}
& e^{ -\frac{1}{x^{2}}}, \quad & x \neq 0 \text{ (infinitely diff'able)} \\
& 0, \quad & x=0
\end{cases}$$
We can prove that this function is smooth and $f^{(n)}(0) = 0$ for all $n.$ Thus, the Maclaurin Series generated by this function is 0. *But* $f(x)\neq 0$ *unless* $x=0.$

Let postpone this issue later. For now, we assume that the Taylor Series **converges** and **equals to** $f(x)$ in some interval.  

# Approximation of Smooth Function by Polynomials  

Assume:  $$f(x) = \sum_{k=0}^{\infty} a_{n}(x-a)^{n}, \quad x \in (a-R, a+R).$$  

## Definition  

> The **Taylor Series of Order n** for Taylor Series generated by $f(x)$ is given by 
> $$P_{n}(x) = f(a) + f'(a)(x-a) + \dots + \frac{f^{(n)}(a)}{n!}(x-a)^{n}= \sum_{k=0}^{n} \frac{f^{(k)}(a)}{n!}(x-a)^{k}.$$  
> If $P_{n}(x) \to f(x),\; \forall\,x,$ then we have  
> $$\lim_{ n \to \infty } P_{n}(x) = f(x).$$  

*Remark:* Studying the polynomial is easier: we can take **derivatives** and **integration** of polynomial without difficulties.  

![[taylor polynomials of cos(x).png]]  

## Convergence of Taylor Series  

We want to answer: 
1. Where does the Taylor Series **converge**?  
2. If converges, does the **sum function equal to** $f(x)$ given?  

Now introduce a theorem, which is the ultimate version of the **Mean Value Theorem** in one-variable calculus.  

### ==Taylor's Theorem==  

> Suppose $f(x)$ has derivatives up to order $n+1$ on a *closed* interval $[a, b]$, then $\exists\,c \in (a, b),$ s.t.  
> $$f(b) = f(a) + f'(a)(b-a) + \dots + \frac{f^{(n)}(a)}{n!}(b-a)^{n} + \symbfit{\frac{f^{(n+1)}(c)}{(n+1)!}(b-a)^{n+1}.}$$  
> *Particularly*, if $n=1$, this is the **Mean Value Theorem**.  

*Remark:* If for a given $x\;(x\neq a),\;R_{n}(x)\to 0$ as $n\to \infty,$ then $\displaystyle f(x) = \lim_{ n \to \infty }f(x) = \lim_{ n \to \infty }P_{n}(x)+\lim_{ n \to \infty }R_{n}(x) = \lim_{ n \to \infty }P_{n}(x) = \sum_{n=0}^{\infty} \frac{f^{(n)}(a)}{n!}(x-a)^{n}.$ Here, $P_{n}(x)\to 0$ gives a **sufficient condition** for the Taylor's Series of $f$ to converge to $f$ (at this $x$).  
$\quad\;$ 

$\underline{Pf}\;$ WLOG, assume $b>a$. Write $P_{n}(x) = \displaystyle \sum_{k=0}^{n} \cfrac{f^{(n)}(a)}{n!}(x-a)^{k}.$ Easy to $\quad\;$ check, we have $P_{n}^{(k)}(a)=f^{(n)}(a).$ *(That's because $P_{n}(a) = \displaystyle\sum_{k=0}^{\infty} \frac{f^{(k)}(a)}{k!}\cdot 1,$ and $P_{n}^{(m)}(a) = \displaystyle \sum_{k=m}^{n} \frac{f^{(k)}(a)}{(k-m)!}\cdot 1$)* 

$\quad\;$ Let $\phi_{n}(x)=f(x)-P_{n}(x)- \frac{f(b)-P_{n}(b)}{(b-a)^{n+1}}(x-a)^{n+1}$ be the difference between $f(x)$ and $P_{n}(x)$. From what we have argued, we can see $\phi_{n}^{(k)}(a)=0$ for all $k\leq n$.  

$\quad\;$ First, we have $\begin{cases} \phi_{n}(a) = f(a) - P_{n}(a) = 0\\ \phi_{n}(b) = f(b)-P_{n}(b) -f(b) + P_{n}(b) =0 \end{cases}$. By **Rolle's MVT**, we can fine a point $c_{1} \in (a,b)$ with $\phi_{n}'(c_{1})=0.$ But by definition, we can check  
$$\phi'_{n}(a)=f'(a)-P_{n}(a)=0; \quad \phi_{n}'(c_{1})=0.$$  
$\quad\;$ Apply **Rolle's Thm** again, we can find some point $c_{2} \in (a, c_{1}).$ Do this $n+1$ times, we finally get some $c:=c_{n+1} \in (a, b),$ s.t.  
$$\phi_{n}^{(n+1)}(c) = f^{(n+1)}(c)- P_{n}^{(n+1)}(c) -(n+1)! \frac{f(b)-P_{n}(b)}{(b-a)^{n+1}}=0.$$  
$\quad\;$ But $P_{n}(x)$ is a *polynomial of order n*, thus $P_{n}^{(n+1)}(c) =0.$ This suggests that  
$$\frac{f(b)-P_{n}(b)}{(b-a)^{n+1}}= \frac{f^{(n+1)}(c)}{(n+1)!}, \quad \text{or} \quad f(b) = f(a) + \frac{f^{(n+1)}(c)}{(n+1)!}(b-a)^{n+1}.$$  
$\quad\;$ Here we can fix $a$ and let $b$ vary. This gives us a general version of **Taylor's Formula**. Basically it is a duplication of the Taylor's Thm.  
$\quad\;$ 


### ==Taylor's Formula== 

> Suppose $f$ ..., then for any *positive integer* $n$ and $x$ in the interval, we have  
> $$f(x) = f(a) + f'(a)(x-a) + \dots + \frac{f^{(n)}(a)}{n!}(x-a)^{n} + R_{n}(x).$$  
> Here, $R_{n}(x)$ is the **Lagrange Remainder** of the Taylor's formula, given by  
> $$R_{n}(x) = \frac{f^{(n+1)}(c)}{(n+1)!}(x-a)^{n+1}, \quad c \text{ between }a \text{ and }x.$$  

Now we want to answer the question raised before. We want the partial sums of the Taylor's Series, or the Taylor Polynomials, converges to $f(x)$, we have  
$$f(x) = P_{n}(x) + R_{n}(x).$$  
If the Remainder converges to 0, then we have the partial sums of the Taylor Polynomials will converge to $f(x)$, as $$\lim_{ n \to \infty } P_{n}(x) = \lim_{ n \to \infty } \big(f(x) -R_{n}(x)\big) = f(x), \quad \forall\,x.$$  

$\quad\;$ 

eg 1:

Show that the Taylor's series of $f(x) = e^{ x }$ centered at 0 converges to itself, $\forall\,x \in \mathbb{R}.$  

$\underline{Pf}\;$ *We need to check if* $R_{n}(x)\to 0\;\forall\,x$. For **fixed** $x,$ we have 
$$R_{n}(x) = \frac{f^{(n+1)}(c)}{(n+1)!}x^{n+1}= \frac{e^{ c }x^{n+1}}{(n+1)!}.$$
$\quad\;$ Note that $x$ is fixed here, $c \in(0,x)$. We have  
$$0\leq |R_{n}(x)| = \Bigg|\frac{e^{ c }x^{n+1}}{(n+1)!}\Bigg| \leq \Bigg|\frac{e^{ |x| }x^{n+1}}{(n+1)!}\Bigg|\to 0.$$
$\quad\;$ By **Sandwich Thm**, $\displaystyle \lim_{ n \to \infty }R_{n}(x) =0.$ Therefore the Taylor's Series of $e^{ x }$ converges to itself, $\forall\,x \in \mathbb{R}.$  

$\quad$  

eg 2:  

Show that the Taylor's series of $f(x) = \sin x$ centered at 0 converges to itself, $\forall\,x \in \mathbb{R}.$  

$\underline{Pf}\;$ For any fixed $x$, we have  
$$0\leq |R_{2n+1}(x)| = \frac{f^{(2n+2)}(c)}{(2n+2)!}x^{2n+1} \leq \frac{x^{2n+2}}{(2n+2)!} \to 0.$$  

Let summarize these two examples, and give a theorem to test if the Taylor Series converges to the function itself.  

### ==The Remainder Estimation Theorem==  

> If the derivatives of $f(x)$ of all orders is **bounded by** some constant $M$ for all points $c$ between $a$ and $x$, then the remainder $R_{n}(x)$ is **bounded by** 
> $$0\leq |R_{n}(x)| \leq M \frac{|x-a|^{n+1}}{(n+1)!}.$$  
> By Sandwich Thm, the remainder converges to 0. Thus, the Taylor's Series at $x$ converges to the function $f(x)$.  



